{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Hello Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em 2017, pesquisadores do Google publicaram um artigo que propunha uma nova arquitetura de rede neural para modelagem de sequência. Apelidada de *Transformer*, essa arquitetura superou as redes neurais recorrentes (RNNs) em tarefas de tradução automática, tanto em termos de qualidade de tradução quanto de custo de treinamento.\n",
    "\n",
    "**Nota:** A. Vaswani et al., “Attention Is All You Need”, (2017). This title was so catchy that no less than 50 follow-up papers have included “all you need” in their titles!\n",
    "\n",
    "Link: https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em paralelo, um método eficaz de aprendizado de transferência chamado ULMFiT mostrou que o treinamento de redes de memória de longo prazo (LSTM) em um corpus muito grande e diversificado poderia produzir classificadores de texto de última geração com poucos dados rotulados.\n",
    "\n",
    "Link:  J. Howard and S. Ruder, “Universal Language Model Fine-Tuning for Text Classification”, (2018).\n",
    "\n",
    "https://arxiv.org/abs/1801.06146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses avanços foram os catalisadores para dois dos transformadores mais conhecidos da atualidade: o Generative Pretrained Transformer (GPT)3 e o Bidirectional Encoder Representations from Transformers (BERT).\n",
    "\n",
    "Link: A. Radford et al., “Improving Language Understanding by Generative Pre-Training”, (2018).\n",
    "\n",
    "https://openai.com/blog/language-unsupervised/\n",
    "\n",
    "\n",
    "J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, (2018).\n",
    "\n",
    "https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao combinar a arquitetura *Transformer* com aprendizado não supervisionado, esses modelos eliminaram a necessidade de treinar arquiteturas específicas de tarefas do zero e quebraram quase todos os benchmarks em PNL por uma margem significativa. Desde o lançamento do GPT e do BERT, surgiu um zoológico de modelos de transformadores.\n",
    "\n",
    "<img src=fig-1-1.PNG>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender o que há de novo nos transformadores, primeiro precisamos explicar:\n",
    "\n",
    "* The encoder-decoder framework\n",
    "\n",
    "* Attention mechanisms\n",
    "\n",
    "* Transfer learning\n",
    "\n",
    "Neste capítulo, será abordado os principais conceitos que fundamentam a difusão dos transformadores, terá um tour por algumas das tarefas em que eles se destacam e concluirá com uma olhada no ecossistema de ferramentas e bibliotecas do Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder-Decoder Framework - (A estrutura do codificador-decodificador)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes dos transformadores, arquiteturas recorrentes como LSTMs eram o estado da arte em PNL. Essas arquiteturas contêm um loop de feedback nas conexões de rede que permitem que as informações se propaguem de uma etapa para outra, tornando-as ideais para modelar dados sequenciais como texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
