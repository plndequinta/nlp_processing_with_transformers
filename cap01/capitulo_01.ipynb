{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1. Hello Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em 2017, pesquisadores do Google publicaram um artigo que propunha uma nova arquitetura de rede neural para modelagem de sequência. Apelidada de *Transformer*, essa arquitetura superou as redes neurais recorrentes (RNNs) em tarefas de tradução automática, tanto em termos de qualidade de tradução quanto de custo de treinamento.\n",
    "\n",
    "**Nota:** A. Vaswani et al., “Attention Is All You Need”, (2017). This title was so catchy that no less than 50 follow-up papers have included “all you need” in their titles!\n",
    "\n",
    "Link: https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em paralelo, um método eficaz de aprendizado de transferência chamado ULMFiT mostrou que o treinamento de redes de memória de longo prazo (LSTM) em um corpus muito grande e diversificado poderia produzir classificadores de texto de última geração com poucos dados rotulados.\n",
    "\n",
    "Link:  J. Howard and S. Ruder, “Universal Language Model Fine-Tuning for Text Classification”, (2018).\n",
    "\n",
    "https://arxiv.org/abs/1801.06146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses avanços foram os catalisadores para dois dos transformadores mais conhecidos da atualidade: o Generative Pretrained Transformer (GPT)3 e o Bidirectional Encoder Representations from Transformers (BERT).\n",
    "\n",
    "Link: A. Radford et al., “Improving Language Understanding by Generative Pre-Training”, (2018).\n",
    "\n",
    "https://openai.com/blog/language-unsupervised/\n",
    "\n",
    "\n",
    "J. Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, (2018).\n",
    "\n",
    "https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao combinar a arquitetura *Transformer* com aprendizado não supervisionado, esses modelos eliminaram a necessidade de treinar arquiteturas específicas de tarefas do zero e quebraram quase todos os benchmarks em PNL por uma margem significativa. Desde o lançamento do GPT e do BERT, surgiu um zoológico de modelos de transformadores.\n",
    "\n",
    "<img src=fig-1-1.PNG>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender o que há de novo nos transformadores, primeiro precisamos explicar:\n",
    "\n",
    "* The encoder-decoder framework\n",
    "\n",
    "* Attention mechanisms\n",
    "\n",
    "* Transfer learning\n",
    "\n",
    "Neste capítulo, será abordado os principais conceitos que fundamentam a difusão dos transformadores, terá um tour por algumas das tarefas em que eles se destacam e concluirá com uma olhada no ecossistema de ferramentas e bibliotecas do Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder-Decoder Framework - (A estrutura do codificador-decodificador)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes dos transformadores, arquiteturas recorrentes como LSTMs eram o estado da arte em PNL. Essas arquiteturas contêm um loop de feedback nas conexões de rede que permitem que as informações se propaguem de uma etapa para outra, tornando-as ideais para modelar dados sequenciais como texto.\n",
    "\n",
    "Conforme ilustrado no lado esquerdo da figura 1-2, um RNN recebe alguma entrada (que pode ser uma palavra ou caractere), a alimenta pela rede e emite um vetor chamado estado oculto.\n",
    "\n",
    "Conforme imagem abaixo, um RNN recebe alguma entrada (que pode ser uma palavra ou caractere), a alimenta pela rede e gera um vetor chamado *hidden state* (estado oculto). Ao mesmo tempo, o modelo alimenta algumas informações de volta para si mesmo por meio do ciclo de *feedback*, que pode ser usado na próxima etapa. Isso pode ser visto com mais clareza se “unroll” o loop conforme mostrado no lado direito, o RNN passa informações sobre seu estado em cada etapa para a próxima operação na sequência.\n",
    "\n",
    "Isso permite que uma RNN acompanhe as informações das etapas anteriores e as use para suas previsões de saída.\n",
    "\n",
    "<img src=fig-1-2.PNG>\n",
    "\n",
    "**Nota:** Essas arquiteturas foram (e continuam sendo) amplamente utilizadas para tarefas de PNL, processamento de fala e séries temporais.\n",
    "\n",
    "**Indicação de leitura:** https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "\n",
    "Uma área em que as RNNs tiveram um papel importante foi no desenvolvimento de sistemas de tradução automática, onde o objetivo é mapear uma sequência de palavras de um idioma para outro.\n",
    "\n",
    "Esse tipo de tarefa geralmente é realizado com um codificador-decodificador ou arquitetura sequência-a-sequência, que é adequada para situações em que a entrada e a saída são ambas sequências de comprimento arbitrário.\n",
    "\n",
    "O trabalho do codificador é codificar as informações da sequência de entrada em uma representação numérica que geralmente é chamada de  *last hidden state* (último estado oculto). Este estado é então passado para o decodificador, que gera a sequência de saída.\n",
    "\n",
    "Em geral, os componentes do codificador e do decodificador podem ser qualquer tipo de arquitetura de rede neural que possa modelar sequências.\n",
    "\n",
    "A imagem mostra que para um par de RNNs, onde a frase em inglês \"Transformers are great!\" é codificado como um vetor de estado oculto que é então decodificado para produzir a tradução alemã “Transformer sind grossartig!”. As palavras de entrada são alimentadas sequencialmente através do codificador e as palavras de saída são geradas uma de cada vez, de cima para baixo.\n",
    "\n",
    "<img src=fig-1-3.PNG>\n",
    "\n",
    "Embora elegante em sua simplicidade, uma fraqueza dessa arquitetura é que o estado oculto final do codificador cria um *information bottleneck* (gargalo de informações): ele tem que representar o significado de toda a sequência de entrada porque isso é tudo a que o decodificador tem acesso ao gerar a saída. Isso é especialmente desafiador para sequências longas, em que as informações no início da sequência podem ser perdidas no processo de compactação de tudo em uma representação única e fixa.\n",
    "\n",
    "Felizmente, existe uma saída para esse gargalo, permitindo que o decodificador tenha acesso a todos os estados ocultos do codificador. O mecanismo geral para isso é chamado de *attention* e é um componente-chave em muitas arquiteturas de redes neurais modernas.\n",
    "\n",
    "Entender como a *attention* foi desenvolvida para RNNs nos colocará em boa forma para entender um dos principais blocos de construção da arquitetura do *Transformer*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia principal por trás da *attention* é que, em vez de produzir um único estado oculto para a sequência de entrada, o codificador emite um estado oculto em cada etapa que o decodificador pode acessar. No entanto, usar todos os estados ao mesmo tempo criaria uma enorme entrada para o decodificador, portanto, algum mecanismo é necessário para priorizar quais estados usar.\n",
    "\n",
    "É aqui que entra a atenção: permite que o decodificador atribua uma quantidade diferente de peso, ou “attention”, a cada um dos estados do codificador em cada passo de tempo de decodificação.\n",
    "\n",
    "Esse processo é ilustrado na Figura 1-4, onde o papel da *attention* é mostrado para prever o terceiro token na sequência de saída.\n",
    "\n",
    "<img src=fig-1-4.PNG>\n",
    "\n",
    "Concentrando-se em quais *tokens* de entrada são mais relevantes em cada passo de tempo, esses modelos baseados em *attention* são capazes de aprender alinhamentos não triviais entre as palavras em uma tradução gerada e aquelas em uma frase de origem.\n",
    "\n",
    "Por exemplo, a Figura 1-5 visualiza os pesos de *attention* para um modelo de tradução de inglês para francês, em que cada pixel denota um peso. A figura mostra como o decodificador consegue alinhar corretamente as palavras “zone” e “Area”, que são ordenadas de forma diferente nos dois idiomas.\n",
    "\n",
    "<img src=fig-1-5.PNG>\n",
    "\n",
    "Embora a *attention* tenha possibilitado a produção de traduções muito melhores, ainda havia uma grande deficiência no uso de modelos recorrentes para o codificador e decodificador: os cálculos são inerentemente sequenciais e não podem ser paralelizados na sequência de entrada.\n",
    "\n",
    "Com o *transformer*, um novo paradigma de modelagem foi introduzido: dispensar totalmente a recorrência e, em vez disso, confiar inteiramente em uma forma especial de atenção chamada *self-attention*. Abordaremos a *self-attention* com mais detalhes no Capítulo 3, mas a ideia básica é permitir que a *attention* opere em todos os estados na mesma camada da rede neural.\n",
    "\n",
    "Isso é mostrado na Figura 1-6, onde o codificador e o decodificador têm seus próprios mecanismos de *self-attention*, cujas saídas são alimentadas para redes neurais de alimentação direta (FF NNs).\n",
    "\n",
    "Essa arquitetura pode ser treinada muito mais rapidamente do que os modelos recorrentes e abriu o caminho para muitos dos avanços recentes em NLP.\n",
    "\n",
    "<img src=fig-1-6.PNG>\n",
    "\n",
    "**Nota:** No artigo original do Transformer, o modelo de tradução foi treinado do zero em um grande corpus de pares de frases em vários idiomas. No entanto, em muitas aplicações práticas da NLP, não temos acesso a grandes quantidades de dados de texto rotulados para treinar nossos modelos. Faltava uma peça final para iniciar a revolução do transformador: o *transfer learning* (aprendizado de transferência)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma prática comum em visão computacional usar o aprendizado de transferência para treinar uma rede neural convolucional como **ResNet** em uma tarefa e depois adaptá-la ou ajustá-la em uma nova tarefa. Isso permite que a rede faça uso do conhecimento aprendido com a tarefa original.\n",
    "\n",
    "Arquitetonicamente, isso envolve dividir o modelo em um *body* e uma *head*, onde a *head* é uma rede específica para tarefas. Durante o treinamento, os pesos do *body* aprendem amplos recursos do domínio de origem e esses pesos são usados ​​para inicializar um novo modelo para a nova tarefa.\n",
    "\n",
    "Em comparação com o aprendizado supervisionado tradicional, essa abordagem normalmente produz modelos de alta qualidade que podem ser treinados com muito mais eficiência em uma variedade de tarefas *downstream* e com muito menos dados rotulados.\n",
    "\n",
    "Uma comparação entre as duas abordagens é mostrada em:\n",
    "\n",
    "<img src=fig-1-7.PNG>\n",
    "\n",
    "Na visão computacional, os modelos são primeiro treinados em conjuntos de dados de grande escala, como o *ImageNet*, que contém milhões de imagens. Esse processo é chamado de pré-treinamento e seu principal objetivo é ensinar aos modelos as características básicas das imagens, como bordas ou cores. Esses modelos pré-treinados podem então ser ajustados em uma tarefa *fine-tuned*, como classificar espécies de flores com um número relativamente pequeno de exemplos rotulados (geralmente algumas centenas por classe). Os modelos ajustados normalmente atingem uma precisão mais alta do que os modelos supervisionados treinados do zero com a mesma quantidade de dados rotulados. \n",
    "\n",
    "Embora o aprendizado por transferência tenha se tornado a abordagem padrão em visão computacional, por muitos anos não ficou claro qual era o processo análogo de pré-treinamento para a NLP. Como resultado, os aplicativos NLP normalmente exigiam grandes quantidades de dados rotulados para obter alto desempenho. E mesmo assim, esse desempenho não se compara ao que foi alcançado no domínio da visão.\n",
    "\n",
    "Em 2017 e 2018, vários grupos de pesquisa propuseram novas abordagens que finalmente fizeram o aprendizado de transferência funcionar para a NLP. Tudo começou com uma visão de pesquisadores da OpenAI que obtiveram forte desempenho em uma tarefa de classificação de sentimentos usando recursos extraídos de pré-treinamento não supervisionado. Isto foi seguido pelo **ULMFiT**, que introduziu uma estrutura geral para adaptar modelos **LSTM** pré-treinados para várias tarefas.\n",
    "\n",
    "Conforme ilustrado na Fig 1-8, o **ULMFiT** envolve três etapas principais:\n",
    "\n",
    "*Pretraining*\n",
    "\n",
    "    *The initial training objective is quite simple: predict the next word based on the previous words. This task is referred to as language modeling. The elegance of this approach lies in the fact that no labeled data is required, and one can make use of abundantly available text from sources such as Wikipedia.*\n",
    "\n",
    "*Domain adaptation*\n",
    "\n",
    "    *Once the language model is pretrained on a large-scale corpus, the next step is to adapt it to the in-domain corpus (e.g., from Wikipedia to the IMDb corpus of movie reviews, as in Figure 1-8). This stage still uses language modeling, but now the model has to predict the next word in the target corpus.*\n",
    "\n",
    "*Fine-tuning*\n",
    "\n",
    "    *In this step, the language model is fine-tuned with a classification layer for the target task (e.g., classifying the sentiment of movie reviews in Figure 1-8).*\n",
    "\n",
    "<img src=fig-1-8.PNG>\n",
    "\n",
    "Ao introduzir uma estrutura viável para pré-treinamento e transferência de aprendizado em NLP, o **ULMFiT** forneceu a peça que faltava para fazer os transformadores decolarem. Em 2018, foram lançados dois transformadores que combinavam *self-attention* com *transfer learning*:\n",
    "\n",
    "    *GPT*\n",
    "\n",
    "        *Uses only the decoder part of the Transformer architecture, and the same language modeling approach as ULMFiT. GPT was pretrained on the BookCorpus,11 which consists of 7,000 unpublished books from a variety of genres including Adventure, Fantasy, and Romance.*\n",
    "    \n",
    "    *BERT*\n",
    "\n",
    "        *Uses the encoder part of the Transformer architecture, and a special form of language modeling called masked language modeling. The objective of masked language modeling is to predict randomly masked words in a text. For example, given a sentence like “I looked at my [MASK] and saw that [MASK] was late.” the model needs to predict the most likely candidates for the masked words that are denoted by [MASK]. BERT was pretrained on the BookCorpus and English Wikipedia.*\n",
    "\n",
    "**GPT** e **BERT** estabeleceram um novo estado da arte em uma variedade de *benchmarks* de NLP e deram início à era dos transformadores.\n",
    "\n",
    "No entanto, com diferentes laboratórios de pesquisa lançando seus modelos em *frameworks* incompatíveis (PyTorch ou TensorFlow), nem sempre foi fácil para os praticantes de NLP portar esses modelos para seus próprios aplicativos. Com o lançamento do *Transformers*, uma API unificada em mais de 50 arquiteturas foi progressivamente construída.\n",
    "\n",
    "Essa biblioteca catalisou a explosão da pesquisa em transformadores e rapidamente chegou aos praticantes de PNL, facilitando a integração desses modelos em muitas aplicações da vida real hoje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers: Bridging the Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar uma nova arquitetura de aprendizado de máquina a uma nova tarefa pode ser uma tarefa complexa e geralmente envolve as seguintes etapas:\n",
    "\n",
    "1. Implemente a arquitetura do modelo em código, normalmente baseado em PyTorch ou TensorFlow.\n",
    "2. Carregue os pesos pré-treinados (se disponíveis) de um servidor.\n",
    "3. Pré-processe as entradas, passe-as pelo modelo e aplique algum pós-processamento específico da tarefa.\n",
    "4. Implemente carregadores de dados e defina funções de perda e otimizadores para treinar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Tour of Transformer Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada tarefa de NLP começa com um pedaço de texto, como o seguinte *feedback* de cliente inventado sobre um determinado pedido online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure\n",
    "from your online store in Germany. Unfortunately, when I opened the package,\n",
    "I discovered to my horror that I had been sent an action figure of Megatron\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do feedback do cliente, você provavelmente gostaria de saber se o *feedback* é positivo ou negativo. Essa tarefa é chamada de análise de sentimento e faz parte do tópico mais amplo de classificação de texto que exploraremos no Capítulo 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como veremos em capítulos posteriores, o Transformers possui uma API em camadas que permite interagir com a biblioteca em vários níveis de abstração. Neste capítulo, começaremos com *pipelines*, que abstraem todas as etapas necessárias para converter texto bruto em um conjunto de previsões de um modelo ajustado.\n",
    "\n",
    "Em Transformers, instanciamos um *pipeline* chamando a função pipeline() e fornecendo o nome da tarefa em que estamos interessados:\n",
    "\n",
    "https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "Downloading: 100%|██████████| 255M/255M [00:13<00:00, 19.1MB/s] \n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 15.8kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 386kB/s]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na primeira vez que você executar esse código, verá algumas barras de progresso aparecerem porque o pipeline baixa automaticamente os pesos do modelo do Hugging Face Hub. Na segunda vez que você instanciar o pipeline, a biblioteca notará que você já baixou os pesos e usará a versão em cache.\n",
    "\n",
    "Por padrão, o pipeline de classificação de texto usa um modelo projetado para análise de sentimentos, mas também oferece suporte à classificação multiclasse e multirótulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "Prever o sentimento do *feedback* do cliente é um bom primeiro passo, mas muitas vezes você quer saber se o *feedback* foi sobre um item ou serviço específico.\n",
    "\n",
    "Em NLP, objetos do mundo real, como produtos, lugares e pessoas, são chamados de *named entities* - entidades nomeadas, e extraí-los do texto é chamado de *named entity recognition* - reconhecimento de entidade nomeada (NER).\n",
    "\n",
    "Podemos aplicar o NER carregando o pipeline correspondente e alimentando nossa análise do cliente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 331kB/s]\n",
      "Downloading: 100%|██████████| 1.24G/1.24G [01:39<00:00, 13.4MB/s] \n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 19.8kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 467kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879010</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556570</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590256</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669693</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879010         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556570           Mega    208  212\n",
       "4          PER  0.590256         ##tron    212  216\n",
       "5          ORG  0.669693         Decept    253  259\n",
       "6         MISC  0.498349        ##icons    259  264\n",
       "7         MISC  0.775362       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que o pipeline detectou todas as entidades e também atribuiu uma categoria como ORG (organização), LOC (local) ou PER (pessoa) a cada uma delas. Aqui usamos o argumento *aggregation_strategy* para agrupar as palavras de acordo com as previsões do modelo.\n",
    "\n",
    "Por exemplo, a entidade “Optimus Prime” é composta por duas palavras, mas é atribuída uma única categoria: MISC (miscellaneous). As pontuações nos dizem o quão confiante o modelo estava sobre as entidades que identificou. Podemos ver que ele estava menos confiante sobre “Decepticons” e a primeira ocorrência de “Megatron”, ambos os quais não conseguiram agrupar como uma única entidade.\n",
    "\n",
    "**Nota:**\n",
    "Veja aqueles estranhos símbolos de hash (#) na coluna de palavras na tabela anterior? Estes são produzidos pelo tokenizer do modelo, que divide as palavras em unidades atômicas chamadas tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "Fornecemos ao modelo uma passagem de texto chamada contexto, juntamente com uma pergunta cuja resposta gostaríamos de extrair. O modelo então retorna o intervalo de texto correspondente à resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 157kB/s]\n",
      "Downloading: 100%|██████████| 249M/249M [00:16<00:00, 15.5MB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 14.5kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 480kB/s]  \n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 592kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que junto com a resposta, o *pipeline* também retornou *strat* e *end* que correspondem aos índices de caracteres onde o intervalo de resposta foi encontrado (assim como com a marcação NER).\n",
    "\n",
    "Existem vários tipos de respostas a perguntas que investigaremos no Capítulo 7, mas esse tipo específico é chamado de *extractive question answering* - resposta a perguntas extrativas porque a resposta é extraída diretamente do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "O objetivo da sumarização de texto é pegar um texto longo como entrada e gerar uma versão curta com todos os fatos relevantes. Esta é uma tarefa muito mais complicada do que as anteriores, pois exige que o modelo gere um texto coerente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Downloading: 100%|██████████| 1.76k/1.76k [00:00<00:00, 594kB/s]\n",
      "Downloading: 100%|██████████| 1.14G/1.14G [01:01<00:00, 19.8MB/s] \n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 5.20kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 1.12MB/s] \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 610kB/s]  \n",
      "Your min_length=56 must be inferior than your max_length=45.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=45, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse resumo não é tão ruim! Embora partes do texto original tenham sido copiadas, o modelo conseguiu captar a essência do problema e identificar corretamente que “Bumblebee” (que aparecia no final) era o autor da denúncia.\n",
    "\n",
    "Neste exemplo, você também pode ver que passamos alguns argumentos de palavras-chave como max_length e clean_up_tokenization_spaces para o pipeline; eles nos permitem ajustar as saídas em tempo de execução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "Assim como a sumarização, a tradução é uma tarefa em que a saída consiste no texto gerado. Vamos usar um *pipeline* de tradução para traduzir um texto em inglês para alemão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.30k/1.30k [00:00<00:00, 190kB/s]\n",
      "Downloading: 100%|██████████| 284M/284M [00:17<00:00, 17.4MB/s] \n",
      "Downloading: 100%|██████████| 42.0/42.0 [00:00<00:00, 10.5kB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\debor\\Dropbox1\\2.Projetos\\nlp_processing_with_transformers\\cap01\\capitulo_01.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/cap01/capitulo_01.ipynb#ch0000041?line=0'>1</a>\u001b[0m translator \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtranslation_en_to_de\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/cap01/capitulo_01.ipynb#ch0000041?line=1'>2</a>\u001b[0m                       model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHelsinki-NLP/opus-mt-en-de\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/cap01/capitulo_01.ipynb#ch0000041?line=2'>3</a>\u001b[0m outputs \u001b[39m=\u001b[39m translator(text, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, min_length\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/cap01/capitulo_01.ipynb#ch0000041?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtranslation_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\debor\\Dropbox1\\2.Projetos\\nlp_processing_with_transformers\\venv_nlp\\lib\\site-packages\\transformers\\pipelines\\__init__.py:598\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=594'>595</a>\u001b[0m             tokenizer_identifier \u001b[39m=\u001b[39m tokenizer\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=595'>596</a>\u001b[0m             tokenizer_kwargs \u001b[39m=\u001b[39m model_kwargs\n\u001b[1;32m--> <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=597'>598</a>\u001b[0m         tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=598'>599</a>\u001b[0m             tokenizer_identifier, revision\u001b[39m=\u001b[39;49mrevision, use_fast\u001b[39m=\u001b[39;49muse_fast, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer_kwargs\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=599'>600</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=601'>602</a>\u001b[0m \u001b[39mif\u001b[39;00m load_feature_extractor:\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=602'>603</a>\u001b[0m     \u001b[39m# Try to infer feature extractor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/pipelines/__init__.py?line=603'>604</a>\u001b[0m     \u001b[39mif\u001b[39;00m feature_extractor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\debor\\Dropbox1\\2.Projetos\\nlp_processing_with_transformers\\venv_nlp\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:551\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=548'>549</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m tokenizer_class_py\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=549'>550</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=550'>551</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=551'>552</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=552'>553</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39min order to use this tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=553'>554</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=555'>556</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=556'>557</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m to build an AutoTokenizer.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=557'>558</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m TOKENIZER_MAPPING\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/debor/Dropbox1/2.Projetos/nlp_processing_with_transformers/venv_nlp/lib/site-packages/transformers/models/auto/tokenization_auto.py?line=558'>559</a>\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_de\",\n",
    "                      model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dac21ce987034d86a3cbfe403b03f86a4b233ff287bf96fdce008fd4ea0804ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
